% no notes
% \documentclass{beamer}
% notes and slides
\documentclass[notes]{beamer}
% notes only
% \documentclass[notes=only]{beamer}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{multirow}
\usepackage{multimedia}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{url}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\usepackage{standalone}
\usepackage{adjustbox}
\usepackage{lmodern}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage{standalone}
\usepackage{csquotes}


\PassOptionsToPackage{american}{babel} % change this to your language(s), main language last
% Spanish languages need extra options in order to work with this template
% \PassOptionsToPackage{spanish,es-lcroman}{babel}
\usepackage{babel}

\PassOptionsToPackage{%
  backend=biber,bibencoding=utf8, %instead of bibtex
  %backend=bibtex8,bibencoding=ascii,%
  language=auto,%
  style=numeric-comp,%
  %style=authoryear-comp, % Author 1999, 2010
  %bibstyle=authoryear,dashed=false, % dashed: substitute rep. author with ---
  style=alphabetic,
  sorting=nyt, % name, year, title
  maxbibnames=10, % default: 3, et al.
  %backref=true,%
  %natbib=true % natbib compatibility mode (\citep and \citet still work)
}{biblatex}
\usepackage{biblatex}

\addbibresource{bib.bib}

\usetheme{metropolis}           % Use metropolis theme
\setbeamertemplate{caption}[default]
\title{Foundations of Machine Learning in Python}
\date{\today}
\institute{High-Performance Computing and Analytics Lab}
\author{Moritz Wolter}

\titlegraphic{\includegraphics[width=2.00cm]{UNI_Bonn_Logo_Standard_RZ.pdf}}

\begin{document}
    \maketitle

    \begin{frame}
    \frametitle{Overview} 
    \tableofcontents
    \end{frame}

    \section{Neural networks}
    \begin{frame}{The wonders of the human visual system}
      \begin{figure}
        \includestandalone[width=0.9\linewidth,height=1.5cm]{./figures/mnist_sequence}
        \caption{Most humans effortlessly recognize the digits \texttt{5 0 4 1 9 2 1 3}.}
      \end{figure}
    \end{frame}

    \begin{frame}{The perceptron}
      How can we teach computers to recognize digits? \\
      Use perceptrons to mimic biological neurons loosely.
      \begin{figure}
        \includestandalone[scale=.575]{./figures/perceptron}
      \end{figure}
      Formally a single perceptron is defined as:
      \begin{align}
        f(\mathbf{w}^T \mathbf{x}) = h
      \end{align}
    \end{frame}

    \begin{frame}{The activation function $f$}
      Two popular choices for the activation function $f$.
      \begin{figure}
        \includestandalone[width=0.49\linewidth]{./figures/sigmoid}
        \includestandalone[width=0.49\linewidth]{./figures/ReLU}
      \end{figure}
    \end{frame}

    \begin{frame}{Arrays of perceptrons}
      Let's extend the definition to cover an array of perceptrons:
      \begin{figure}
        \includestandalone[scale=0.5]{./figures/perceptron_array}
      \end{figure}
      Every input is connected to every neuron, in matrix language, this turns into
      \begin{align}
        \bar{\mathbf{h}} = \mathbf{W}\mathbf{x} + \mathbf{b} \\
        \mathbf{h} = f(\bar{\mathbf{h}}).
      \end{align}
    \end{frame}

    \begin{frame}{The loss function}
      To choose weights for the network, we require a quality measure. \\
      We already saw the mean squared error cost function,
      \begin{align}
        C_{\text{mse}} = \frac{1}{2} \sum_{k=1}^{n} (\mathbf{y}_k - \mathbf{h}_k)^2 = \frac{1}{2} (\mathbf{y} - \mathbf{h})^T(\mathbf{y} - \mathbf{h})
      \end{align}
      This function measures the squared distance from each desired output.
      $\mathbf{y}$ denotes the desired labels, and $\mathbf{h}$ the network output.
    \end{frame}


    \begin{frame}{The gradient of the mse-cost-function}
      Both the mean squared error loss function and our dense layer are differentiable. 
      \begin{align}
        \frac{\partial C_{\text{mse}}}{\partial \mathbf{o}} = \mathbf{o} - \mathbf{y} = \triangle_{\text{mse}}
      \end{align}
      The $\triangle$ symbol will re-appear. It always indicates incoming gradient information from above.
    \end{frame}

    \begin{frame}{The gradient of a dense layer}
      The chain rule tells us the gradients for the dense layer\cite{nielsen2015neural}
      \begin{align}
        \delta \mathbf{W} = [f'(\bar{\mathbf{h}})]\mathbf{x}^T, \dots
      \end{align}
      Good news! Jax can take care of these computations for you!
      \note{
        You can choose to optimally verify these equations by completing the deep learning project.}
    \end{frame}

    \begin{frame}{Derivatives of our activation functions}
      TODO
    \end{frame}

    \begin{frame}{Perceptrons can learn functions}
      TODO
    \end{frame}

    \begin{frame}{Multi-layer networks}
      TODO
    \end{frame}

    \begin{frame}{Backpropagation}
      TODO
    \end{frame}

    \section{Estimation, Overfitting and Regularization}
    \begin{frame}{Denoising a signal}
      TODO
    \end{frame}

    \section{Classification}

    \begin{frame}{The cross-entropy loss}
      TODO
    \end{frame}

    \begin{frame}{MNIST digit }
      Modified National Institute of Standards and Technology database
        \cite{dumoulin2016guide}
    \end{frame}

    \begin{frame}{Literature}
      \printbibliography
    \end{frame}

\end{document}
