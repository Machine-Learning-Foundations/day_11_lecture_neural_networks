% no notes
% \documentclass{beamer}
% notes and slides
\documentclass[notes]{beamer}
% notes only
% \documentclass[notes=only]{beamer}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{multirow}
\usepackage{multimedia}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{url}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\usepackage{standalone}
\usepackage{adjustbox}
\usepackage{lmodern}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage{standalone}
\usepackage{csquotes}
%\usepackage{hyperlink}
%\usepackage{url}


\PassOptionsToPackage{american}{babel} % change this to your language(s), main language last
% Spanish languages need extra options in order to work with this template
% \PassOptionsToPackage{spanish,es-lcroman}{babel}
\usepackage{babel}

\PassOptionsToPackage{%
  backend=biber,bibencoding=utf8, %instead of bibtex
  %backend=bibtex8,bibencoding=ascii,%
  language=auto,%
  style=numeric-comp,%
  %style=authoryear-comp, % Author 1999, 2010
  %bibstyle=authoryear,dashed=false, % dashed: substitute rep. author with ---
  style=alphabetic,
  sorting=nyt, % name, year, title
  maxbibnames=10, % default: 3, et al.
  %backref=true,%
  %natbib=true % natbib compatibility mode (\citep and \citet still work)
}{biblatex}
\usepackage{biblatex}

\addbibresource{bib.bib}

\usetheme{metropolis}           % Use metropolis theme
\setbeamertemplate{caption}[default]
\title{Introduction to Neural Networks in Python}
\date{\today}
\institute{High-Performance Computing and Analytics Lab}
\author{Moritz Wolter}

\titlegraphic{\includegraphics[width=2.00cm]{UNI_Bonn_Logo_Standard_RZ.pdf}}

\begin{document}
    \maketitle

    \begin{frame}
    \frametitle{Overview} 
    \tableofcontents
    \end{frame}

    \section{Neural networks}
    \begin{frame}{The wonders of the human visual system}
      \begin{figure}
        \includestandalone[width=0.9\linewidth,height=1.5cm]{./figures/mnist_sequence}
        \caption{Most humans effortlessly recognize the digits \texttt{5 0 4 1 9 2 1 3}.}
      \end{figure}
    \end{frame}

    \begin{frame}{Biological motivation}
      \centering
      \begin{figure}
        \includegraphics[scale=0.3]{./figures/Chemical_synapse_schema_cropped}  
      \end{figure}
      Image source: en.wikipedia.org
      %Image source: \url{https://en.wikipedia.org/wiki/Neuron#/media/File:Chemical_synapse_schema_cropped.jpg} 
    \note{
      \begin{itemize}
        \item A Human brain contains approximately 86 billion neurons.
        \item $10^{14}$ to $10^{15}$ synapses connect these neurons.
        \item Neurons recieve inputs from dentrites.
        \item and can produce outputs signals along its axon.
        \item Axons are connect neurons, modelled by weighting inputs $wx$.
        \item Neuron inputs can be inhibitive (negative weight) or
        \item excitory (positive weight).
        \item If enough inputs exite a neuron it fires.
        \item The activation function aims to mimic this behaviour.
        \item Even though neural networks started out as biologically motivated,
        \item engineering efforts have since diverged from biology.
      \end{itemize}
    }
    \end{frame}


    \begin{frame}{The perceptron}
      Can computers recognize digits? Mimic biological neurons,
      \begin{figure}
        \includestandalone[scale=.575]{./figures/perceptron}
      \end{figure}
      Formally a single perceptron is defined as
      \begin{align}
        f(\mathbf{w}^T \mathbf{x} + b) = h
      \end{align}
      with $\mathbf{w} \in \mathbb{R}^n$, $\mathbf{x} \in \mathbb{R}^n$ and $h,b \in \mathbb{R}$. 
    \end{frame}

    \begin{frame}{The activation function $f$}
      Two popular choices for the activation function $f$.
      \begin{figure}
        \includestandalone[width=0.49\linewidth]{./figures/sigmoid}
        \includestandalone[width=0.49\linewidth]{./figures/ReLU}
      \end{figure}
    \end{frame}

    \begin{frame}{Arrays of perceptrons}
      Let's extend the definition to cover an array of perceptrons:
      \begin{figure}
        \includestandalone[scale=0.5]{./figures/perceptron_array}
      \end{figure}
      Every input is connected to every neuron. In matrix language, this turns into
      \begin{align}
        \bar{\mathbf{h}} &= \mathbf{W}\mathbf{x} + \mathbf{b}, & \mathbf{h} = f(\bar{\mathbf{h}}).
      \end{align}
      With $\mathbf{W} \in \mathbb{R}^{m,n}$, $\mathbf{x} \in \mathbb{R}^{n}$, $\mathbf{b} \in \mathbb{R}^{m}$, and $\mathbf{h}, \bar{\mathbf{h}} \in \mathbb{R}^m$.
    \end{frame}

    \begin{frame}{The loss function}
      To choose weights for the network, we require a quality measure. \\
      We already saw the mean squared error cost function,
      \begin{align}
        C_{\text{mse}} = \frac{1}{2} \sum_{k=1}^{n} (\mathbf{y}_k - \mathbf{h}_k)^2 = \frac{1}{2} (\mathbf{y} - \mathbf{h})^T(\mathbf{y} - \mathbf{h})
      \end{align}
      This function measures the squared distance from each desired output.
      $\mathbf{y}$ denotes the desired labels, and $\mathbf{h}$ represents network output.
    \end{frame}


    \begin{frame}{The gradient of the mse-cost-function}
      Both the mean squared error loss function and our dense layer are differentiable. 
      \begin{align}
        \frac{\partial C_{\text{mse}}}{\partial \mathbf{h}} = \mathbf{h} - \mathbf{y} = \triangle_{\text{mse}}
      \end{align}
      The $\triangle$ symbol will re-appear. It always indicates incoming gradient information from above.
      If the labels are a vector of shape $\mathbb{R}^m$, $\triangle$ and the network output $\mathbf{h}$ must share 
      this dimension.
    \end{frame}

    \begin{frame}{The gradient of a dense layer}
      The chain rule tells us the gradients for the dense layer~\cite{nielsen2015neural}
      \begin{align}
        \delta \mathbf{W} &= [f'(\bar{\mathbf{h}}) \odot \triangle]\mathbf{x}^T, &  \delta \mathbf{b} = f'(\bar{\mathbf{h}}) \odot \triangle, \\
        \delta \mathbf{x} &= \mathbf{W}^T [f'(\bar{\mathbf{h}}) \odot \triangle],
      \end{align}
      where $\odot$ is the element-wise product. $\delta$ denotes the cost function gradient for the value following it \cite{greff2016lstm}. \\
      \textbf{Modern libraries will take care of these computations for you!} \\
      You can choose to verify these equations yourself by completing the optional deep learning project.
      \note{
        On the board, derive:
        Recall the chain rule $(g(h(x)))' = g'(h(x)) \cdot h'(x)$.
        For the activation function, we have,
        \begin{align}
          \bar{\mathbf{h}} &= f(\bar{\mathbf{h}}) \\
          \Rightarrow \delta \bar{\mathbf{h}} &= f'(\bar{\mathbf{h}}) \odot \triangle
        \end{align}
        For the weight matrix,
        \begin{align}
          \bar{\mathbf{h}} &= \mathbf{W}\mathbf{x} + \mathbf{b} \\
          \Rightarrow \delta \mathbf{W} &= \delta \bar{\mathbf{h}} \mathbf{x}^T = [f'(\bar{\mathbf{h}}) \odot \triangle]^T \mathbf{x}
        \end{align}
        For the bias,
        \begin{align}
          \bar{\mathbf{h}}  &= \mathbf{W}\mathbf{x} + \mathbf{b} \\
          \Rightarrow \delta \mathbf{b} &= 1 \odot \delta \bar{\mathbf{h}} = [f'(\bar{\mathbf{h}}) \odot \triangle]
        \end{align}
      }
    \end{frame}

    \begin{frame}{Derivatives of our activation functions}
      \begin{align}
        \sigma'(x) = \sigma \cdot (1 - \sigma(x)) \\
        \text{ReLU}' = H(x) 
      \end{align}
      \begin{figure}
        \includestandalone[width=0.49\linewidth]{./figures/sigmoid_prime}
        \includestandalone[width=0.447\linewidth]{./figures/relu_prime}
      \end{figure}
    \end{frame}

    \begin{frame}{Perceptrons for functions}
      The network components described this far already allow function learning.
      Given a noisy input signal $x \in \mathbb{R}^{m}$ and a ground through output $y \in \mathbb{R}^m$,
      define,
      \begin{align}
        \mathbf{h} = \sigma(\mathbf{W}\mathbf{x} + \mathbf{b}) \\
        \mathbf{y}_{\text{net}} = \mathbf{W}_y\mathbf{h}
      \end{align}
      With $\mathbf{W} \in \mathbb{R}^{m,n}$, $\mathbf{x} \in \mathbb{R}^{n}$ and $\mathbf{b} \in \mathbb{R}^{m}$.
      $m$ and $n$ denote the number of neurons and the input signal length.
      For signal denoising, input and output have the same length. Therefore $\mathbf{W}_y \in \mathbb{R}^{n,m}$.
    \end{frame}

    \begin{frame}{Denoising a cosine}
      Training works by iteratively descending along the gradients. For $\mathbf{W}$ the weights at the
      next time step $\tau$ are given by,
      \begin{align}
        \mathbf{W}_{\tau + 1} = \mathbf{W}_\tau + \epsilon \cdot \delta\mathbf{W}_{\tau}.
      \end{align}
      The step size is given by $\epsilon$. At $\tau = 0$ matrix entries are random.
      $\mathcal{U}[-0.1, 0.1]$ is a reasonable choice here.
      The process is the same for all other network components.
    \end{frame}


    \begin{frame}{Denoising a cosine}
      Optimization for 500 steps leads to the output below:
      \begin{figure}
        \includestandalone[width=.5\linewidth]{./figures/denoising}
        \caption{The cosine function is shown in blue. A noisy network input in orange, and a denoised network output in green.}
        \end{figure}
    \end{frame}

    \begin{frame}{Summary}
      \begin{itemize}
        \item Artificial neural networks are biologically motivated.
        \item Gradients make it possible to optimize arrays of neurons.
        \item A single array of layer of neurons can solve tasks like denoising a sine.
      \end{itemize}
    \end{frame}


    \section{Classification with neural networks}

    \begin{frame}{Deep multi-layer networks}
      Stack dense layers and activations to create deep networks. \\
      \begin{figure}
        \includestandalone[width=\linewidth]{./figures/multilayer} 
      \end{figure}
    \end{frame}

    \begin{frame}{Backpropagation}
      \begin{figure}
        \includestandalone[width=\linewidth]{./figures/backprop} 
      \end{figure}
    \end{frame}

    \begin{frame}{The cross-entropy loss}
      The cross entropy loss function is defined as \cite{nielsen2015neural, bishop2006pattern}
      \begin{align} \label{eq:ce}
       C_{\text{ce}}(\mathbf{y}, \mathbf{o}) = -\sum_k^{n_o} [( \mathbf{y}_k  \ln \mathbf{o}_k) 
                                  + (\mathbf{1} - \mathbf{y}_k)
                                     \ln(\mathbf{1} - \mathbf{o}_k)].
      \end{align}
    \end{frame}

    \begin{frame}{Understanding how cross entropy works}
      To understand cross entropy lets consider the boundary cases $y=0$ and $y=1$.
      \begin{figure}
        \includestandalone[width=0.49\linewidth]{./figures/ce_label_0}
        \includestandalone[width=0.49\linewidth]{./figures/ce_label_1}
      \end{figure}
    \end{frame}

    \begin{frame}{Gradients and cross-entropy}
      If a sigmoidal activation function produced $\mathbf{o}$ the gradients can be computed using ~\cite{nielsen2015neural,bishop2006pattern}
      \begin{align} 
         \frac{\partial C_{ce}}{\partial \mathbf{h}} 
         = \sigma(\mathbf{o}) - \mathbf{y} = \triangle_{\text{ce}}
      \end{align}
      \note{
        Following \cite{nielsen2015neural}, substitute $\sigma(\mathbf{o})$ into eq~\ref{eq:ce}.
      }
    \end{frame}

    \begin{frame}{The MNIST-Dataset}
      \begin{figure}
        \includestandalone[width=0.9\linewidth,height=1.5cm]{./figures/mnist_sequence}
        \caption{The MNIST-dataset contains 70k images of handwritten digits.}
      \end{figure}
    \end{frame}

    \begin{frame}{Validation and Test data splits}
      \begin{itemize}
        \item To ensure the correct operation of the systems we devise, it is paramount to 
        hold back part of the data for validation and testing.
        \item Before starting to train, split off validation and test data.
        \item The 70k MNIST samples could, for example, be partitioned into 59k training images.
        1k validation images and 10k test images. 
      \end{itemize}
    \end{frame}

    \begin{frame}{Input-preprocessing}
      Standard initializations and learning algorithms assume an approximately standard normal distribution
      of the network inputs. Consequently we must rescale the data using,
      \begin{align}
        \mathbf{X}_n = \frac{\mathbf{X} - \mu}{\sigma}
      \end{align}
      With $\mu$ and $\sigma$ the training set mean and standard deviation. And the data matrix $\mathbf{X} \in \mathbb{R}^{b,n}$ with a row for each data point.
      $b$ denotes the number of data points and $n$ the data dimension.
    \end{frame}

    \begin{frame}{The effect of normalization}
      \begin{figure}
      \includegraphics[width=\linewidth]{./figures/normalized.pdf}
      \end{figure}
    \end{frame}

    \begin{frame}{Whitening the inputs}
      Instead of deviding by the standard deviation, rescale the centered data with the singular values of the covariance matrix.
      \begin{align}
        \mathbf{C} = \frac{1}{n} (\mathbf{X} - \mu)^T(\mathbf{X} - \mu)
      \end{align}
      With $n$ as the total number of data points.
      Whitening now uses the singular values of $\mathbf{C}$ to rescale the data,
      \begin{align}
        \mathbf{X}_w = \frac{(\mathbf{X} - \mu)}{\sqrt{\sigma} + \epsilon}
      \end{align}
      With $\epsilon$ i.e. equal to $1e^{-8}$ for numerical stability.
    \end{frame}

    \begin{frame}{The effect of Whitening}
    \begin{figure}
    \includegraphics[width=\linewidth]{./figures/whitened.pdf}
      \end{figure}
      Whitening is expensive and sometimes unstable. Consequently, most projects rely on normalization.
    \end{frame}


    \begin{frame}{Label-encoding}
      It has proven useful to have individual output neurons produce probabilities for each class.
      Given integer labels $1,2,3,4, \dots \in \mathbb{Z}$. One-hot encoded label vectors have a one 
      at the labels positions and zeros elsewhere. I.e.
      \begin{align}
        \begin{pmatrix}
          1 \\ 0 \\ 0 \\ 0 \\ \vdots
        \end{pmatrix},
        \begin{pmatrix}
          0 \\ 1 \\ 0 \\ 0 \\\vdots
        \end{pmatrix},
        \begin{pmatrix}
          0 \\ 0 \\ 1 \\ 0 \\\vdots
        \end{pmatrix},
        \begin{pmatrix}
          0 \\ 0 \\ 0 \\ 1 \\\vdots
        \end{pmatrix},
        \dots
      \end{align}
      for the integer label sequence above.
    \end{frame}

    \begin{frame}{MNIST-Classification}
      Training a three-layer dense network on mnist for five runs leads to:
      \begin{figure}
        \includestandalone[width=0.7\linewidth]{./figures/mnist_runs_plot}
      \end{figure}
    \end{frame}

    \begin{frame}{Conclusion}
      \begin{itemize}
        \item Preprocessing followed by forward-passes, backward-passes, and testing form the classic training pipeling.
        \item Using the pipeline, artificial neural networks enable computers to make sense of images.
        \item The optimization result depends on the initialization.
        \item The initialization depends on the pseudo-randomness-seed.
        \item \textit{Seed-values must be recorded}, to allow reproduction.
        \item Share the results of multiple re-initialized runs, if possible.
      \end{itemize}
    \end{frame}

    \begin{frame}[allowframebreaks]{Literature}
      \printbibliography
    \end{frame}

\end{document}
